<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aura AI Companion</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        #chat-log::-webkit-scrollbar { width: 4px; }
        #chat-log::-webkit-scrollbar-thumb { background-color: #4A5568; border-radius: 20px; }
        .fade-in { animation: fadeIn 0.5s ease-in-out forwards; }
        .fade-out { animation: fadeOut 0.5s ease-in-out forwards; }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }
        @keyframes fadeOut { from { opacity: 1; } to { opacity: 0; } }
        .spinner {
            border: 4px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top: 4px solid #6366F1;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
        .icon-button { cursor: pointer; transition: color 0.2s; }
        .icon-button:hover { color: #a78bfa; }
        .mic-button.is-listening { color: #ef4444; animation: pulse 1.5s infinite; }
        @keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.1); } 100% { transform: scale(1); } }
        .speak-button.active { color: #6366F1; }
    </style>
</head>
<body class="bg-gray-800 text-gray-200 flex flex-col h-screen">

    <div id="loading-screen" class="absolute inset-0 bg-gray-900 flex flex-col items-center justify-center z-50 transition-opacity duration-500">
        <div class="spinner"></div>
        <p id="loading-text" class="mt-4 text-lg">Initializing Aura AI...</p>
    </div>

    <div id="app-container" class="flex flex-col h-screen opacity-0" style="visibility: hidden;">
        <header class="bg-gray-900 p-4 shadow-md">
            <h1 class="text-xl font-bold text-center">Aura AI Companion</h1>
        </header>

        <main class="flex-1 flex flex-col md:flex-row p-4 gap-4 overflow-hidden">
            <div class="w-full md:w-1/3 lg:w-1/4 flex flex-col items-center bg-gray-900 p-4 rounded-lg shadow-lg space-y-4">
                <div class="relative w-full max-w-xs aspect-square rounded-lg overflow-hidden border-2 border-gray-700">
                    <video id="webcam" autoplay muted playsinline class="w-full h-full object-cover"></video>
                    <canvas id="overlay" class="absolute top-0 left-0"></canvas>
                </div>
                <div id="status" class="text-center p-3 bg-gray-700 rounded-md w-full max-w-xs">
                    <p class="text-lg">Current Emotion: <span id="emotion-status" class="font-semibold text-indigo-400">Neutral</span></p>
                </div>
                <div class="text-left p-3 bg-gray-800 rounded-md w-full max-w-xs h-24 border border-gray-700">
                    <p class="text-sm font-semibold text-gray-400 mb-1">System Analysis:</p>
                    <p id="robotic-text" class="text-xs text-gray-300 font-mono">Awaiting facial data stream...</p>
                </div>
            </div>
            <div class="flex-1 flex flex-col bg-gray-900 rounded-lg shadow-lg overflow-hidden">
                <div id="chat-log" class="flex-1 p-4 space-y-4 overflow-y-auto">
                    <div class="flex items-start gap-3">
                        <div class="bg-indigo-500 p-2 rounded-full text-white text-sm flex-shrink-0">AI</div>
                        <div class="bg-gray-700 p-3 rounded-lg"><p>Hello! I'm Aura. My systems are online.</p></div>
                    </div>
                </div>
                <div class="p-4 bg-gray-800 border-t border-gray-700">
                    <form id="chat-form" class="flex items-center gap-3">
                        <div class="relative flex-1">
                            <input type="text" id="message-input" placeholder="Type or click the mic to speak..." class="w-full bg-gray-700 border border-gray-600 rounded-lg p-3 pr-10 focus:outline-none focus:ring-2 focus:ring-indigo-500" autocomplete="off" disabled>
                            <div id="mic-button" class="mic-button icon-button absolute inset-y-0 right-0 flex items-center pr-3 text-gray-400">
                                <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16"><path d="M5 3a3 3 0 0 1 6 0v5a3 3 0 0 1-6 0V3z"/><path d="M3.5 6.5A.5.5 0 0 1 4 7v1a4 4 0 0 0 8 0V7a.5.5 0 0 1 1 0v1a5 5 0 0 1-4.5 4.975V15h3a.5.5 0 0 1 0 1h-7a.5.5 0 0 1 0-1h3v-2.025A5 5 0 0 1 3 8V7a.5.5 0 0 1 .5-.5z"/></svg>
                            </div>
                        </div>
                        <button type="submit" id="send-button" class="bg-indigo-600 hover:bg-indigo-700 text-white font-bold py-3 px-4 rounded-lg transition-colors" disabled>Send</button>
                        <div id="speak-button" class="speak-button icon-button text-gray-400 p-2 rounded-full hover:bg-gray-700">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16"><path d="M11.536 14.01A8.473 8.473 0 0 0 14.026 8a8.473 8.473 0 0 0-2.49-6.01l-1.414 1.414A6.472 6.472 0 0 1 12.025 8a6.472 6.472 0 0 1-1.903 4.596l1.414 1.414zM10.121 12.596A6.472 6.472 0 0 0 12.025 8a6.472 6.472 0 0 0-1.904-4.596l-1.414 1.414A4.472 4.472 0 0 1 10.025 8a4.472 4.472 0 0 1-1.317 3.182l1.414 1.414zM8.707 11.182A4.472 4.472 0 0 0 10.025 8a4.472 4.472 0 0 0-1.318-3.182L7.293 6.232A2.5 2.5 0 0 1 8.025 8a2.5 2.5 0 0 1-.732 1.768l1.414 1.414zM6 7.5a.5.5 0 0 1 .5-.5h1a.5.5 0 0 1 0 1h-1a.5.5 0 0 1-.5-.5z"/></svg>
                        </div>
                    </form>
                </div>
            </div>
        </main>
    </div>

    <script>
        // --- DOM ELEMENTS ---
        const loadingScreen = document.getElementById('loading-screen'), loadingText = document.getElementById('loading-text');
        const appContainer = document.getElementById('app-container'), video = document.getElementById('webcam');
        const emotionStatus = document.getElementById('emotion-status'), chatLog = document.getElementById('chat-log');
        const chatForm = document.getElementById('chat-form'), messageInput = document.getElementById('message-input');
        const sendButton = document.getElementById('send-button'), micButton = document.getElementById('mic-button');
        const speakButton = document.getElementById('speak-button'), roboticText = document.getElementById('robotic-text');
        const canvas = document.getElementById('overlay');

        // --- GLOBAL STATE ---
        let currentEmotion = 'Neutral';
        let isSpeakModeEnabled = false;
        // THE ONLY CHANGE IS ON THIS LINE: Using the live Render URL
        const backendUrl = 'https://aura-ai-backend-1.onrender.com';
        const roboticPhrases = {
            'Happy': "Analysis: Zygomaticus major activity detected. Probability of positive sentiment: 92%.",
            'Sad': "Processing... Corrugator supercilii contraction indicates negative valence. User state: Subdued.",
            'Angry': "Warning: Orbicularis oculi tension levels high. Anger signature detected.",
            'Surprise': "Facial markers indicate unexpected stimuli. Levator palpebrae superioris activated.",
            'Fear': "Alert: Amygdala response indicators present. Elevated fear markers detected.",
            'Disgust': "Levator labii superioris activated. Disgust signature identified.",
            'Neutral': "System nominal. Facial expression within baseline parameters."
        };

        // --- BROWSER APIs ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;
        const speechSynthesis = window.speechSynthesis;

        // --- INITIALIZATION ---
        async function initialize() {
            try {
                // Step 1: Check if the backend server is online
                loadingText.textContent = 'Connecting to Aura AI server...';
                await fetch(`${backendUrl}/status`);
                
                // Step 2: Load the face detection models for the frontend
                loadingText.textContent = 'Loading face detection models...';
                await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
                
                // Step 3: Request webcam access
                loadingText.textContent = 'Requesting webcam access...';
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;

                // Step 4: Once the video is ready, fade out the loader and show the app
                video.onloadedmetadata = () => {
                    loadingScreen.classList.add('fade-out');
                    appContainer.style.visibility = 'visible';
                    appContainer.classList.add('fade-in');
                    setTimeout(() => loadingScreen.style.display = 'none', 500);
                };

            } catch (err) {
                console.error("Initialization failed:", err);
                loadingText.textContent = 'Error: Could not start. Is the server running?';
                loadingScreen.querySelector('.spinner').style.display = 'none';
            }
        }

        // --- CORE FUNCTIONS ---
        function speak(text) {
            if (!isSpeakModeEnabled || !speechSynthesis) return;
            speechSynthesis.cancel(); 
            const utterance = new SpeechSynthesisUtterance(text);
            speechSynthesis.speak(utterance);
        }

        async function predictAndTrigger(faceImage) {
            try {
                const response = await fetch(`${backendUrl}/predict_and_trigger`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ image: faceImage })
                });
                if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
                const data = await response.json();
                
                if (data.emotion !== currentEmotion) {
                    currentEmotion = data.emotion;
                    emotionStatus.textContent = currentEmotion;
                    roboticText.classList.remove('fade-in');
                    void roboticText.offsetWidth; // Trigger reflow
                    roboticText.textContent = roboticPhrases[currentEmotion] || "Analyzing...";
                    roboticText.classList.add('fade-in');
                }

                if (data.ai_response) {
                    appendMessage('ai', data.ai_response);
                    messageInput.disabled = false;
                    sendButton.disabled = false;
                    messageInput.focus();
                }
            } catch (error) { console.error('Error in predict/trigger:', error); }
        }

        async function sendChatMessage(message) {
            appendMessage('user', message);
            messageInput.value = '';
            try {
                const response = await fetch(`${backendUrl}/chat`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ message: message, emotion: currentEmotion })
                });
                if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
                const data = await response.json();
                appendMessage('ai', data.reply);
            } catch (error) {
                console.error('Error sending chat message:', error);
                appendMessage('ai', 'Sorry, I am having trouble connecting.');
            }
        }
        
        function appendMessage(sender, text) {
            const senderClass = sender === 'user' ? 'bg-gray-600' : 'bg-indigo-500';
            const senderName = sender === 'user' ? 'You' : 'AI';
            const messageDiv = document.createElement('div');
            messageDiv.className = 'flex items-start gap-3' + (sender === 'user' ? ' flex-row-reverse' : '');
            messageDiv.innerHTML = `<div class="${senderClass} p-2 rounded-full text-white text-sm flex-shrink-0">${senderName}</div><div class="bg-gray-700 p-3 rounded-lg max-w-xs md:max-w-md"><p>${text.replace(/\n/g, '<br>')}</p></div>`;
            chatLog.appendChild(messageDiv);
            chatLog.scrollTop = chatLog.scrollHeight;
            if (sender === 'ai') speak(text);
        }

        // --- EVENT LISTENERS ---
        video.addEventListener('play', () => {
            const displaySize = { width: video.clientWidth, height: video.clientHeight };
            faceapi.matchDimensions(canvas, displaySize);

            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions());
                if (detections.length > 0) {
                    const face = detections[0].box;
                    const faceCanvas = document.createElement('canvas');
                    faceCanvas.width = face.width; faceCanvas.height = face.height;
                    const faceCtx = faceCanvas.getContext('2d');
                    faceCtx.drawImage(video, face.x, face.y, face.width, face.height, 0, 0, face.width, face.height);
                    const faceImageData = faceCanvas.toDataURL('image/jpeg');
                    predictAndTrigger(faceImageData);
                }
            }, 2000);
        });

        chatForm.addEventListener('submit', (e) => {
            e.preventDefault();
            const message = messageInput.value.trim();
            if (message) sendChatMessage(message);
        });
        
        speakButton.addEventListener('click', () => {
            isSpeakModeEnabled = !isSpeakModeEnabled;
            speakButton.classList.toggle('active', isSpeakModeEnabled);
            if (isSpeakModeEnabled) {
                speak("Voice enabled.");
            } else {
                speechSynthesis.cancel(); // Stop any ongoing speech
            }
        });

        // Mic Button Listener
        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.continuous = false; recognition.lang = 'en-US';
            micButton.addEventListener('click', () => {
                if (micButton.classList.contains('is-listening')) { recognition.stop(); return; }
                try {
                    recognition.start();
                } catch(e) { console.error("Mic could not start: ", e); }
            });
            recognition.onstart = () => {
                micButton.classList.add('is-listening');
                messageInput.placeholder = "Listening...";
            }
            recognition.onresult = (event) => { messageInput.value = event.results[0][0].transcript; };
            recognition.onerror = (event) => { console.error('Mic error:', event.error); };
            recognition.onend = () => {
                micButton.classList.remove('is-listening');
                messageInput.placeholder = "Type or click the mic to speak...";
            };
        } else { micButton.style.display = 'none'; }

        // --- START THE APP ---
        initialize();
    </script>
</body>
</html>
